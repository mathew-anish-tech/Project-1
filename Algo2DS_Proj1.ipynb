{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FHdvhf0alpxv"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def rate_url_validity(user_query: str, url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluates the validity of a given URL by computing various metrics including\n",
        "    domain trust, content relevance, fact-checking, bias, and citation scores.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's original query.\n",
        "        url (str): The URL to analyze.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing scores for different validity aspects.\n",
        "    \"\"\"\n",
        "\n",
        "    # === Step 1: Fetch Page Content ===\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        page_text = \" \".join([p.text for p in soup.find_all(\"p\")])  # Extract paragraph text\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Failed to fetch content: {str(e)}\"}\n",
        "\n",
        "    # === Step 2: Domain Authority Check (Moz API) ===\n",
        "    # Replace with actual Moz API call\n",
        "    domain_trust = 60  # Placeholder value (Scale: 0-100)\n",
        "\n",
        "    # === Step 3: Content Relevance (Semantic Similarity using Hugging Face) ===\n",
        "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "    similarity_score = util.pytorch_cos_sim(model.encode(user_query), model.encode(page_text)).item() * 100\n",
        "\n",
        "    # === Step 4: Fact-Checking (Google Fact Check API) ===\n",
        "    fact_check_score = check_facts(page_text)\n",
        "\n",
        "    # === Step 5: Bias Detection (NLP Sentiment Analysis) ===\n",
        "    sentiment_pipeline = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "    sentiment_result = sentiment_pipeline(page_text[:512])[0]  # Process first 512 characters\n",
        "    bias_score = 100 if sentiment_result[\"label\"] == \"POSITIVE\" else 50 if sentiment_result[\"label\"] == \"NEUTRAL\" else 30\n",
        "\n",
        "    # === Step 6: Citation Check (Google Scholar via SerpAPI) ===\n",
        "    citation_count = check_google_scholar(url)\n",
        "    citation_score = min(citation_count * 10, 100)  # Normalize\n",
        "\n",
        "    # === Step 7: Compute Final Validity Score ===\n",
        "    final_score = (\n",
        "        (0.3 * domain_trust) +\n",
        "        (0.3 * similarity_score) +\n",
        "        (0.2 * fact_check_score) +\n",
        "        (0.1 * bias_score) +\n",
        "        (0.1 * citation_score)\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"Domain Trust\": domain_trust,\n",
        "        \"Content Relevance\": similarity_score,\n",
        "        \"Fact-Check Score\": fact_check_score,\n",
        "        \"Bias Score\": bias_score,\n",
        "        \"Citation Score\": citation_score,\n",
        "        \"Final Validity Score\": final_score\n",
        "    }"
      ],
      "metadata": {
        "id": "-a9ahP2wmW-w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Helper Function: Fact-Checking via Google API ===\n",
        "def check_facts(text: str) -> int:\n",
        "    \"\"\"\n",
        "    Cross-checks text against Google Fact Check API.\n",
        "    Returns a score between 0-100 indicating factual reliability.\n",
        "    \"\"\"\n",
        "    api_url = f\"https://toolbox.google.com/factcheck/api/v1/claimsearch?query={text[:200]}\"\n",
        "    try:\n",
        "        response = requests.get(api_url)\n",
        "        data = response.json()\n",
        "        if \"claims\" in data and data[\"claims\"]:\n",
        "            return 80  # If found in fact-checking database\n",
        "        return 40  # No verification found\n",
        "    except:\n",
        "        return 50  # Default uncertainty score"
      ],
      "metadata": {
        "id": "YR5urkg2msUb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_google_scholar(url: str) -> int:\n",
        "    \"\"\"\n",
        "    Checks Google Scholar citations using SerpAPI.\n",
        "    Returns the count of citations found.\n",
        "    \"\"\"\n",
        "    serpapi_key = \"YOUR_SERPAPI_KEY\"\n",
        "    params = {\"q\": url, \"engine\": \"google_scholar\", \"api_key\": serpapi_key}\n",
        "    try:\n",
        "        response = requests.get(\"https://serpapi.com/search\", params=params)\n",
        "        data = response.json()\n",
        "        return len(data.get(\"organic_results\", []))\n",
        "    except:\n",
        "        return -1  # Assume no citations found"
      ],
      "metadata": {
        "id": "rUbJ1ffBm0LB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"I have just been on an international flight, can I come back home to hold my 1-month-old newborn?\"\n",
        "url_to_check =\"https://www.quora.com/How-soon-can-I-take-my-newborn-with-me-when-I-fly-internationally\"\n",
        "\n",
        "result = rate_url_validity(user_prompt, url_to_check)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QA3Lc3QmegT",
        "outputId": "15fba297-c783-4199-8321-b2d2d7228c8f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Domain Trust': 60, 'Content Relevance': -4.416078701615334, 'Fact-Check Score': 50, 'Bias Score': 30, 'Citation Score': 0, 'Final Validity Score': 29.6751763895154}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Az5ZvPgwmfCN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}